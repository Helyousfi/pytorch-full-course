{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_pytorch_course.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP60ActanOYjZPbDP6KFkCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Helyousfi/pytorch-full-course/blob/main/full_pytorch_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a simple Neural Network :"
      ],
      "metadata": {
        "id": "-cQpqgb5f4-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import libraries :"
      ],
      "metadata": {
        "id": "YRjHrnYIgE-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAvAURwOfnwq"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torch.nn.functional as F # All functions that don't have any parameters\n",
        "from torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\n",
        "import torchvision.datasets as datasets # Has standard datasets we can import in a nice and easy way\n",
        "import torchvision.transforms as transforms # Transformations we can perform on our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the dataset :"
      ],
      "metadata": {
        "id": "P_w8d9pSi2ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "XyZ_8mb6gp1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build The Model:"
      ],
      "metadata": {
        "id": "qrKujZ-ni_q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 4096)\n",
        "    self.fc2 = nn.Linear(4096, num_classes)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "8nhhHNishVhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 784 # 28x28 = 784, size of MNIST images (grayscale)\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "39Arc1mCkHaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(input_size, 10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PD_1Ui9rkMHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "eAxN09uImOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch : {epoch}\")\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    # Get data to cuda if possible\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Get to correct shape, 28x28->784\n",
        "    # -1 will flatten all outer dimensions into one\n",
        "    data = data.reshape(data.shape[0], -1) \n",
        "\n",
        "    predicted = model(data)\n",
        "    loss = criterion(predicted, targets)\n",
        "    \n",
        "\n",
        "    # zero previous gradients\n",
        "    optimizer.zero_grad()\n",
        "        \n",
        "    # back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient descent or adam step\n",
        "    optimizer.step()\n",
        "  print(loss)\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08kxWosFmMtz",
        "outputId": "ab5d76bf-e50e-49c7-8ca3-01fae81a379b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 1\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 2\n",
            "tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmOOen5ynmLc",
        "outputId": "194b2f1b-8eb9-4fbb-f47e-7a1adf157e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (fc1): Linear(in_features=784, out_features=4096, bias=True)\n",
              "  (fc2): Linear(in_features=4096, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(model, loader):\n",
        "\n",
        "  if loader.dataset.train:\n",
        "    print(\"Training data\")\n",
        "  else:\n",
        "    print(\"Testing data\")\n",
        "  corrects = 0\n",
        "  samples = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        predicted = model(data.reshape(data.shape[0], -1)).to(device)\n",
        "        _, predicted = predicted.max(1)\n",
        "\n",
        "        for i in range(len(predicted)):\n",
        "          if predicted[i] == target[i]:\n",
        "            corrects = corrects + 1\n",
        "          samples = samples + 1\n",
        "      print(f\"Got {corrects} corrects / {samples} samples with accuracy {corrects / samples}\")\n",
        "  return corrects / samples\n",
        "        \n",
        "check_accuracy(model, train_loader)\n",
        "check_accuracy(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBAfoI4Srbdx",
        "outputId": "9fe02300-ca5a-4276-d96f-bf301518d6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data\n",
            "Got 54452 corrects / 60000 samples with accuracy 0.9075333333333333\n",
            "Training data\n",
            "Got 54452 corrects / 60000 samples with accuracy 0.9075333333333333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9075333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a CNN : Convolutional Neural Network"
      ],
      "metadata": {
        "id": "AZIpUKwFxTv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some Imports : "
      ],
      "metadata": {
        "id": "vrdNvAko3kvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torch.nn.functional as F # All functions that don't have any parameters\n",
        "from torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\n",
        "import torchvision.datasets as datasets # Has standard datasets we can import in a nice and easy way\n",
        "import torchvision.transforms as transforms # Transformations we can perform on our dataset"
      ],
      "metadata": {
        "id": "ajx4cSjJrdpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the dataset:"
      ],
      "metadata": {
        "id": "Gtg4tHQv3npp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "DGjDqLp_3qDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the model:"
      ],
      "metadata": {
        "id": "GaA48q-I3say"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_model(nn.Module):\n",
        "  def __init__(self, in_channels=1, num_classes=10):\n",
        "    super(CNN_model, self).__init__()\n",
        "    self.Conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=2, padding=1)\n",
        "    self.MaxPool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "    self.Conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.Conv1(x))\n",
        "    x = self.MaxPool(x)\n",
        "    x = F.relu(self.Conv2(x))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x) \n",
        "    return x\n",
        "\n",
        "def test():\n",
        "  model = CNN_model(in_channels=1, num_classes=10)\n",
        "  input = torch.rand(1, 1, 28, 28)\n",
        "  out = model(input)\n",
        "  print(out) \n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ6KlC2-xday",
        "outputId": "cf6ca265-48ef-402c-e4e9-54ff37f3b00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0191, -0.0080, -0.0558, -0.0045, -0.0122, -0.0333,  0.0698, -0.0256,\n",
            "         -0.0025,  0.0192]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "hnD__9dSzzLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 784 # 28x28 = 784, size of MNIST images (grayscale)\n",
        "in_channels = 1\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 3\n",
        "load_model = False"
      ],
      "metadata": {
        "id": "8iajW79B2gtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_model(in_channels, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TNM1RBrL2pz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM5bVkbx20uQ",
        "outputId": "a0545639-d4d6-4520-dc04-5612c0c8e380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_model(\n",
              "  (Conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (MaxPool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the Model"
      ],
      "metadata": {
        "id": "NV9Er_PN26AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save and Load a model :"
      ],
      "metadata": {
        "id": "Gf_H3eWK7CAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoints(checkpoint, filename=\"model.pth.tar\"):\n",
        "  print(\"------------Saving the checkpoints------------\")\n",
        "  torch.save(checkpoint, filename)\n",
        "def load_checkpoints(checkpoint):\n",
        "  print(\"------------Loading the checkpoints------------\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "if load_model:\n",
        "  model = CNN_model().to(device)\n",
        "  checkpoint = torch.load(\"model.pth.tar\")\n",
        "  load_checkpoints(checkpoint)\n",
        "  print(model)"
      ],
      "metadata": {
        "id": "0n-naohb5zro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train the model"
      ],
      "metadata": {
        "id": "fcJEWsY48qqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch : {epoch}\")\n",
        "  losses = []\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    checkpoint = {'state_dict' : model.state_dict(), 'optimizer' : optimizer.state_dict()}\n",
        "    save_checkpoints(checkpoint)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    # Get data to cuda if possible\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "    # Get to correct shape, 28x28->784\n",
        "    # -1 will flatten all outer dimensions into one\n",
        "    #data = data.reshape(data.shape[0], -1) \n",
        "    predicted = model(data)\n",
        "    loss = criterion(predicted, targets)\n",
        "    # zero previous gradients\n",
        "    optimizer.zero_grad()\n",
        "    # back-propagation\n",
        "    loss.backward()\n",
        "    # gradient descent or adam step\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8w8hDS825Wx",
        "outputId": "5ef00337-e638-4cef-b1e3-7e0ca0611d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "------------Saving the checkpoints------------\n",
            "tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 1\n",
            "------------Saving the checkpoints------------\n",
            "tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 2\n",
            "------------Saving the checkpoints------------\n",
            "tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(model, loader):\n",
        "\n",
        "  if loader.dataset.train:\n",
        "    print(\"Training data : \")\n",
        "  else:\n",
        "    print(\"Testing data : \")\n",
        "  corrects = 0\n",
        "  samples = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        predicted = model(data).to(device)\n",
        "        _, predicted = predicted.max(1)\n",
        "\n",
        "        for i in range(len(predicted)):\n",
        "          if predicted[i] == target[i]:\n",
        "            corrects = corrects + 1\n",
        "          samples = samples + 1\n",
        "      print(f\"Got {corrects} corrects / {samples} samples with accuracy {corrects / samples}\")\n",
        "  return corrects / samples\n",
        "        \n",
        "check_accuracy(model, train_loader)\n",
        "check_accuracy(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m9FcXTo2_sE",
        "outputId": "faefb23e-c2e3-4f9e-ccea-44cc1e5b6395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data : \n",
            "Got 58849 corrects / 60000 samples with accuracy 0.9808166666666667\n",
            "Training data : \n",
            "Got 58849 corrects / 60000 samples with accuracy 0.9808166666666667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9808166666666667"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the convolutional layers increased the accuracy "
      ],
      "metadata": {
        "id": "1A7lOpET3x2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-oM1divI66jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "gLtgfOSmeaPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mzCJjCZMeckL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(pretrained=True)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIzmTJwye2zx",
        "outputId": "df674d74-0deb-40f5-f150-bc3255aff486"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Identity, self).__init__()\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.avgpool = Identity()\n",
        "model.classifier = nn.Linear(512, 10) # Cifar10 32x32 so 32/2**5 = 1 \n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bItwZRmye8i5",
        "outputId": "d8f63358-43e1-4ecb-edec-ad0137f4f6cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): Identity()\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=\"dataset/\",\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"dataset/\",\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_63IF6NfrnO",
        "outputId": "ab8075af-ad54-481b-866c-0f3a76d52329"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch : {epoch}\")\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    # Get data to cuda if possible\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "    # Get to correct shape, 28x28->784\n",
        "    # -1 will flatten all outer dimensions into one\n",
        "    #data = data.reshape(data.shape[0], -1) \n",
        "    predicted = model(data)\n",
        "    loss = criterion(predicted, targets)\n",
        "    # zero previous gradients\n",
        "    optimizer.zero_grad()\n",
        "    # back-propagation\n",
        "    loss.backward()\n",
        "    # gradient descent or adam step\n",
        "    optimizer.step()\n",
        "\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXGXFPSOiY0q",
        "outputId": "00193714-d8de-4586-e2c1-ee7d24fa9ca9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "tensor(1.5860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 1\n",
            "tensor(1.2527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch : 2\n",
            "tensor(1.2932, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(model, loader):\n",
        "  if loader.dataset.train:\n",
        "    print(\"Training data : \")\n",
        "  else:\n",
        "    print(\"Testing data : \")\n",
        "  corrects = 0\n",
        "  samples = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        predicted = model(data).to(device)\n",
        "        _, predicted = predicted.max(1)\n",
        "\n",
        "        for i in range(len(predicted)):\n",
        "          if predicted[i] == target[i]:\n",
        "            corrects = corrects + 1\n",
        "          samples = samples + 1\n",
        "      print(f\"Got {corrects} corrects / {samples} samples with accuracy {corrects / samples}\")\n",
        "  return corrects / samples\n",
        "        \n",
        "check_accuracy(model, train_loader)\n",
        "check_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg_iXVJRjlod",
        "outputId": "f0d6543d-79e3-454c-e146-fb2b15c72fa3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data : \n",
            "Got 27783 corrects / 50000 samples with accuracy 0.55566\n",
            "Training data : \n",
            "Got 27783 corrects / 50000 samples with accuracy 0.55566\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55566"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}